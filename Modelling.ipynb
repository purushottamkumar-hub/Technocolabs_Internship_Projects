{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# For regular expressions\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# For missing values\n",
    "import missingno as msg\n",
    "#For datetime\n",
    "import datetime\n",
    "# For handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>author</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>archived</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>edited</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>nb_chars</th>\n",
       "      <th>nb_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most of us have some family members like this....</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>14</td>\n",
       "      <td>YoungModern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>exmormon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>family member like family like</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But Mill's career was way better. Bentham is l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>3</td>\n",
       "      <td>RedCoatsForever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CanadaPolitics</td>\n",
       "      <td>on</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>mill's career way well bentham like joseph smi...</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mine uses a strait razor, and as much as i lov...</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>1</td>\n",
       "      <td>vhisic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>AdviceAnimals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>mine use strait razor much love clipper love r...</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>109</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very fast, thank you!</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>2</td>\n",
       "      <td>Mastersimpson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>freedonuts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>fast thank</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The guy is a professional, and very good at wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>6</td>\n",
       "      <td>BigGupp1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>WTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>guy professional good highly doubt miss often</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.1953</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  downs  created_utc  \\\n",
       "0  Most of us have some family members like this....      0   1420070400   \n",
       "1  But Mill's career was way better. Bentham is l...      0   1420070400   \n",
       "2  Mine uses a strait razor, and as much as i lov...      0   1420070400   \n",
       "3                              Very fast, thank you!      0   1420070400   \n",
       "4  The guy is a professional, and very good at wh...      0   1420070400   \n",
       "\n",
       "   score           author distinguished  archived       subreddit  \\\n",
       "0     14      YoungModern           NaN     False        exmormon   \n",
       "1      3  RedCoatsForever           NaN     False  CanadaPolitics   \n",
       "2      1           vhisic           NaN     False   AdviceAnimals   \n",
       "3      2    Mastersimpson           NaN     False      freedonuts   \n",
       "4      6         BigGupp1           NaN     False             WTF   \n",
       "\n",
       "  author_flair_css_class author_flair_text  ...  ups  controversiality  \\\n",
       "0                    NaN               NaN  ...   14               0.0   \n",
       "1                     on           Ontario  ...    3               0.0   \n",
       "2                    NaN               NaN  ...    1               0.0   \n",
       "3                    NaN               NaN  ...    2               0.0   \n",
       "4                    NaN               NaN  ...    6               0.0   \n",
       "\n",
       "   edited                                         body_clean  \\\n",
       "0    True                     family member like family like   \n",
       "1    True  mill's career way well bentham like joseph smi...   \n",
       "2    True  mine use strait razor much love clipper love r...   \n",
       "3    True                                         fast thank   \n",
       "4    True      guy professional good highly doubt miss often   \n",
       "\n",
       "  positive_sentiment  neutral_sentiment  negative_sentiment  \\\n",
       "0              0.625              0.375               0.000   \n",
       "1              0.338              0.662               0.000   \n",
       "2              0.363              0.563               0.074   \n",
       "3              0.714              0.286               0.000   \n",
       "4              0.251              0.346               0.404   \n",
       "\n",
       "   compound_sentiment  nb_chars  nb_words  \n",
       "0              0.6124        30         5  \n",
       "1              0.5574        69        11  \n",
       "2              0.8481       109        21  \n",
       "3              0.3612        10         2  \n",
       "4             -0.1953        45         7  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the engineered dataset\n",
    "\n",
    "df=pd.read_csv(\"engineered_train.csv\",encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc2vec vector columns\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df[\"body_clean\"].apply(lambda x: str(x).split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# transform each document into a vector data\n",
    "doc2vec_df = df[\"body_clean\"].apply(lambda x: model.infer_vector(str(x).split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "model_df = pd.concat([df, doc2vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step consist in extracting vector representations for every comment. The module Gensim creates a numerical vector representation of every word in the corpus by using the contexts in which they appear (Word2Vec). This is performed using shallow neural networks. What's interesting is that similar words will have similar representation vectors.\n",
    "\n",
    "Each text can also be transformed into numerical vectors using the word vectors (Doc2Vec). Same texts will also have similar representations and that is why we can use those vectors as training features.\n",
    "\n",
    "We first have to train a Doc2Vec model by feeding in our text data. By applying this model on our comment, we can get those representation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>author</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>archived</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>nb_chars</th>\n",
       "      <th>nb_words</th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>doc2vec_vector_1</th>\n",
       "      <th>doc2vec_vector_2</th>\n",
       "      <th>doc2vec_vector_3</th>\n",
       "      <th>doc2vec_vector_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most of us have some family members like this....</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>14</td>\n",
       "      <td>YoungModern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>exmormon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>0.055995</td>\n",
       "      <td>0.076622</td>\n",
       "      <td>0.069184</td>\n",
       "      <td>0.064952</td>\n",
       "      <td>0.040338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But Mill's career was way better. Bentham is l...</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>3</td>\n",
       "      <td>RedCoatsForever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CanadaPolitics</td>\n",
       "      <td>on</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "      <td>0.037877</td>\n",
       "      <td>0.218856</td>\n",
       "      <td>-0.094495</td>\n",
       "      <td>0.043705</td>\n",
       "      <td>0.207068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mine uses a strait razor, and as much as i lov...</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>1</td>\n",
       "      <td>vhisic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>AdviceAnimals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>109</td>\n",
       "      <td>21</td>\n",
       "      <td>0.084206</td>\n",
       "      <td>-0.016747</td>\n",
       "      <td>0.020130</td>\n",
       "      <td>0.109601</td>\n",
       "      <td>0.011560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very fast, thank you!</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>2</td>\n",
       "      <td>Mastersimpson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>freedonuts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035912</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>-0.071770</td>\n",
       "      <td>0.055905</td>\n",
       "      <td>0.005814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The guy is a professional, and very good at wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>6</td>\n",
       "      <td>BigGupp1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>WTF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.1953</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>0.105689</td>\n",
       "      <td>-0.009693</td>\n",
       "      <td>0.033573</td>\n",
       "      <td>-0.068896</td>\n",
       "      <td>-0.197156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  downs  created_utc  \\\n",
       "0  Most of us have some family members like this....      0   1420070400   \n",
       "1  But Mill's career was way better. Bentham is l...      0   1420070400   \n",
       "2  Mine uses a strait razor, and as much as i lov...      0   1420070400   \n",
       "3                              Very fast, thank you!      0   1420070400   \n",
       "4  The guy is a professional, and very good at wh...      0   1420070400   \n",
       "\n",
       "   score           author distinguished  archived       subreddit  \\\n",
       "0     14      YoungModern           NaN     False        exmormon   \n",
       "1      3  RedCoatsForever           NaN     False  CanadaPolitics   \n",
       "2      1           vhisic           NaN     False   AdviceAnimals   \n",
       "3      2    Mastersimpson           NaN     False      freedonuts   \n",
       "4      6         BigGupp1           NaN     False             WTF   \n",
       "\n",
       "  author_flair_css_class author_flair_text  ...  neutral_sentiment  \\\n",
       "0                    NaN               NaN  ...              0.375   \n",
       "1                     on           Ontario  ...              0.662   \n",
       "2                    NaN               NaN  ...              0.563   \n",
       "3                    NaN               NaN  ...              0.286   \n",
       "4                    NaN               NaN  ...              0.346   \n",
       "\n",
       "   negative_sentiment  compound_sentiment  nb_chars nb_words  \\\n",
       "0               0.000              0.6124        30        5   \n",
       "1               0.000              0.5574        69       11   \n",
       "2               0.074              0.8481       109       21   \n",
       "3               0.000              0.3612        10        2   \n",
       "4               0.404             -0.1953        45        7   \n",
       "\n",
       "   doc2vec_vector_0  doc2vec_vector_1  doc2vec_vector_2  doc2vec_vector_3  \\\n",
       "0          0.055995          0.076622          0.069184          0.064952   \n",
       "1          0.037877          0.218856         -0.094495          0.043705   \n",
       "2          0.084206         -0.016747          0.020130          0.109601   \n",
       "3          0.035912          0.041719         -0.071770          0.055905   \n",
       "4          0.105689         -0.009693          0.033573         -0.068896   \n",
       "\n",
       "   doc2vec_vector_4  \n",
       "0          0.040338  \n",
       "1          0.207068  \n",
       "2          0.011560  \n",
       "3          0.005814  \n",
       "4         -0.197156  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178525, 26)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting, Cross Validation and Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split , cross_val_score , RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler ,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"score\"\n",
    "ignore_cols = [label, \"body\", \"body_clean\",'author', 'author_flair_css_class', 'author_flair_text', 'subreddit','distinguished']\n",
    "features = [c for c in model_df.columns if c not in ignore_cols]\n",
    "# to impute any missing values\n",
    "it=IterativeImputer(random_state= 100)\n",
    "model_df[features]=pd.DataFrame(it.fit_transform(model_df[features]))\n",
    "\n",
    "X=model_df[features]\n",
    "y=model_df[label]\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first choose which features we want to use to train our model. Then we split our data into two parts:\n",
    "\n",
    "1. one to train our model\n",
    "2. one to assess its performances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(rf ,X,y, cv =10 ,scoring = 'accuracy',n_jobs =-1 , verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45331317 0.47879908 0.46586008 0.4538733  0.45342519 0.46062066\n",
      " 0.45462693 0.48252297 0.48319516 0.45731571]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4643552247881312\n"
     ]
    }
   ],
   "source": [
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc of RandomForest without HyperParameter tuning is  0.9398123512113149\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc of RandomForest without HyperParameter tuning is \" , accuracy_score(rf_pred , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "result1 = cross_val_score(xgb ,X,y, cv =10 ,scoring = 'accuracy',n_jobs =-1 , verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.fit(X_train , y_train))\n",
    "xgb_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acc of XGBClassifier without HyperParameter tuning is \" , accuracy_score(xgb_pred , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParam tuning for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param = {'n_estimators':list(range(100,500)) , \n",
    "         'max_depth':list(range(1,10)) , \n",
    "         'criterion':['gini','entropy'] ,\n",
    "         'max_samples':list(range(1,10))    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(rf ,param_distributions=rf_param ,  cv =5 , n_iter=10 , scoring = 'accuracy',n_jobs =-1 , verbose =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  50 | elapsed: 16.6min remaining:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  50 | elapsed: 19.4min remaining:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed: 20.8min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 21.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                      9],\n",
       "                                        'max_samples': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                        9],\n",
       "                                        'n_estimators': [100, 101, 102, 103,\n",
       "                                                         104, 105, 106, 107,\n",
       "                                                         108, 109, 110, 111,\n",
       "                                                         112, 113, 114, 115,\n",
       "                                                         116, 117, 118, 119,\n",
       "                                                         120, 121, 122, 123,\n",
       "                                                         124, 125, 126, 127,\n",
       "                                                         128, 129, ...]},\n",
       "                   scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46145357793026187\n",
      "RandomForestClassifier(criterion='entropy', max_depth=5, max_samples=9,\n",
      "                       n_estimators=228)\n",
      "7\n",
      "{'n_estimators': 228, 'max_samples': 9, 'max_depth': 5, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "print(rscv.best_score_)\n",
    "print()\n",
    "print(rscv.best_estimator_)\n",
    "print()\n",
    "print(rscv.best_index_)\n",
    "print()\n",
    "print(rscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_depth=5, max_samples=9,\n",
      "                       n_estimators=228)\n",
      "Accuracy score of RF after tuning is : 0.49105167343509315\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "rf = RandomForestClassifier(criterion='entropy', max_depth=5, max_samples=9,\n",
    "                       n_estimators=228)\n",
    "print(rf.fit(X_train , y_train))\n",
    "print(\"Accuracy score of RF after tuning is :\", accuracy_score(rf.predict(X_test) , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParam Tuning for XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {'n_estimators':list(range(100,500)) , \n",
    "         'max_depth':list(range(1,10)) , \n",
    "         'learning_rate':[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.05,0.09] ,\n",
    "         'min_child_weight ':list(range(1,10))    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(xgb ,param_distributions=xgb_param ,  cv =5 , n_iter=10 , scoring = 'accuracy',n_jobs =-1 , verbose =10)\n",
    "rscv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rscv.best_score_)\n",
    "print()\n",
    "print(rscv.best_estimator_)\n",
    "print()\n",
    "print(rscv.best_index_)\n",
    "print()\n",
    "print(rscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict\n",
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.09, max_delta_step=0, max_depth=2,\n",
    "               min_child_weight =9,\n",
    "              monotone_constraints='()', n_estimators=448, n_jobs=0,\n",
    "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
    "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
    "              validate_parameters=1, verbosity=None)\n",
    "\n",
    "print(xgb.fit(X_train , y_train))\n",
    "print(\"Accuracy score of XGB after tuning is :\", accuracy_score(xgb.predict(X_test) , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "It is completely possible to use only raw text as input for making predictions. The most important thing is to be able to extract the relevant features from this raw source of data. This kind of data can often come as a good complementary source in data science projects in order to extract more learning features and increase the predictive power of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
